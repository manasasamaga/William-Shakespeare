{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text generation using lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZInavL2H03R4"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf#to build the model\n",
        "import string#to get set of punctuations\n",
        "import requests#to get the data file in the notebook"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x_6R5Bp1Zth"
      },
      "source": [
        "response = requests.get('https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt')\n",
        "#The get() method sends a GET request to the specified url. Here we are sending a request to get the text document of the data."
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AkKXKOx1gun",
        "outputId": "3e3e1156-b9ff-4ea0-c07c-932efc33ad98"
      },
      "source": [
        "print(\"tensorflow version{}\".format(tf.__version__))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow version2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "k_5K2cbJ1qKF",
        "outputId": "339d3dde-72c0-49cd-f984-c589adc1125c"
      },
      "source": [
        "response.text[:1500]#display some part of the text returned by requests.get()."
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is the 100th Etext file presented by Project Gutenberg, and\\nis presented in cooperation with World Library, Inc., from their\\nLibrary of the Future and Shakespeare CDROMS.  Project Gutenberg\\noften releases Etexts that are NOT placed in the Public Domain!!\\n\\nShakespeare\\n\\n*This Etext has certain copyright implications you should read!*\\n\\n<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM\\nSHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS\\nPROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE COLLEGE\\nWITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE\\nDISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS\\nPERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\\nCOMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\\nSERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\\n\\n*Project Gutenberg is proud to cooperate with The World Library*\\nin the presentation of The Complete Works of William Shakespeare\\nfor your reading for education and entertainment.  HOWEVER, THIS\\nIS NEITHER SHAREWARE NOR PUBLIC DOMAIN. . .AND UNDER THE LIBRARY\\nOF THE FUTURE CONDITIONS OF THIS PRESENTATION. . .NO CHARGES MAY\\nBE MADE FOR *ANY* ACCESS TO THIS MATERIAL.  YOU ARE ENCOURAGED!!\\nTO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALLOWED!!\\n\\n\\n**Welcome To The World of Free Plain Vanilla Electronic Texts**\\n\\n**Etexts Readable By Both Humans and By Computers, Since 1971**\\n\\n*These Etexts Prepared By Hundreds of Volunteers and Donations*\\n\\nInforma'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Chc1-v7r6WeF",
        "outputId": "f972f600-6333-458c-f485-d9a416877e14"
      },
      "source": [
        "data = response.text.split('\\n')\n",
        "data[0]\n",
        "#You can see the character \\n in the text. \\n means “newline”. Now we are going to split the text with respect to \\n."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is the 100th Etext file presented by Project Gutenberg, and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8PxTJ3ck8bPR",
        "outputId": "82f4eb8d-eabe-49fb-8d66-e82d8b07266d"
      },
      "source": [
        "data = data[253:]\n",
        "data[0]\n",
        "#The text file contains a header file before the actual data begins. The actual work of william shakesphere  begins from line 253. So we are going to slice the data and retain everything from line 253 onwards."
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  From fairest creatures we desire increase,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlRsW-cl8tAm",
        "outputId": "77c1efce-4c5e-4f6a-8c13-1b72fddd8615"
      },
      "source": [
        "len(data)\n",
        "#The total number of lines in our data is 124204"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "6VlGl46r8zt0",
        "outputId": "d85d8d98-b821-46aa-f7f2-9b7323ef9b5a"
      },
      "source": [
        "data = \" \".join(data)\n",
        "data[:1000]\n",
        "#join all the lines and create a long string consisting of the data in continuous format"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"  From fairest creatures we desire increase,   That thereby beauty's rose might never die,   But as the riper should by time decease,   His tender heir might bear his memory:   But thou contracted to thine own bright eyes,   Feed'st thy light's flame with self-substantial fuel,   Making a famine where abundance lies,   Thy self thy foe, to thy sweet self too cruel:   Thou that art now the world's fresh ornament,   And only herald to the gaudy spring,   Within thine own bud buriest thy content,   And tender churl mak'st waste in niggarding:     Pity the world, or else this glutton be,     To eat the world's due, by the grave and thee.                        2   When forty winters shall besiege thy brow,   And dig deep trenches in thy beauty's field,   Thy youth's proud livery so gazed on now,   Will be a tattered weed of small worth held:     Then being asked, where all thy beauty lies,   Where all the treasure of thy lusty days;   To say within thine own deep sunken eyes,   Were an all\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0usRxTN87TJ",
        "outputId": "cc180f59-60fd-4e31-c6c8-ca8d7c3f3b32"
      },
      "source": [
        "def clean_text(doc):#to remove all the punctuation marks and special characters\n",
        "  tokens = doc.split()#split the data according to space character \n",
        "  table = str.maketrans('', '', string.punctuation)# characters that need to be deleted from the string\n",
        "  tokens = [w.translate(table) for w in tokens]#translate the characters in the string \n",
        "  tokens = [word for word in tokens if word.isalpha()]#isalpha() method returns True if all the characters are alphabet letters \n",
        "  tokens = [word.lower() for word in tokens]#lower() methods returns the lowercased string\n",
        "  return tokens\n",
        "\n",
        "tokens = clean_text(data)\n",
        "print(tokens[:50])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that', 'thereby', 'beautys', 'rose', 'might', 'never', 'die', 'but', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', 'his', 'tender', 'heir', 'might', 'bear', 'his', 'memory', 'but', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', 'feedst', 'thy', 'lights', 'flame', 'with', 'selfsubstantial', 'fuel', 'making', 'a', 'famine', 'where', 'abundance', 'lies', 'thy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui6UCpk7-bo3",
        "outputId": "f80f276c-4345-4908-b2ce-10fa74d98288"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "898199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5jbALzl-eRz",
        "outputId": "ba9699e3-a54f-4d63-bb30-120605b7bcd5"
      },
      "source": [
        "len(set(tokens))# total number of unique words are 27956"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27956"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyBWpq4o_YiB",
        "outputId": "68aab19a-ffb8-4b8e-8ece-290f49600956"
      },
      "source": [
        "#I set of 50 words to predict the 51st word\n",
        "#divide data in chunks of 51 words and at the last we will separate the last word from every line\n",
        "length = 50 + 1#50 previous as input and next 1 for output\n",
        "lines = []\n",
        "\n",
        "for i in range(length, len(tokens)):\n",
        "  seq = tokens[i-length:i]#0 to 50 \n",
        "  line = ' '.join(seq)\n",
        "  lines.append(line)\n",
        "  if i > 200000:#Since its a huge dataset RAM will overflow so considering only in limited\n",
        "    break\n",
        "\n",
        "print(len(lines))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "199951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "-I9b3puxCCKK",
        "outputId": "36402bc7-2db9-4661-8129-eb3a48bbca1c"
      },
      "source": [
        "lines[0]#first line consisting of 51 words"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from fairest creatures we desire increase that thereby beautys rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feedst thy lights flame with selfsubstantial fuel making a famine where abundance lies thy self'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a-mFe3GTCKmt",
        "outputId": "1b5a38f2-335d-4686-f39b-ad080a702dd8"
      },
      "source": [
        "tokens[50]#51st word in this line is 'self'"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'self'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "HbSbIOI0DAvj",
        "outputId": "cab2685c-b215-449b-cd54-d4a81b7d7617"
      },
      "source": [
        "lines[1]#51st word in this line is 'thy' which will the output word used for prediction"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fairest creatures we desire increase that thereby beautys rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feedst thy lights flame with selfsubstantial fuel making a famine where abundance lies thy self thy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-1gy_kFDJX3"
      },
      "source": [
        "import numpy as np#arrays\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer#generating dictionary of word encoding and creating vectors out of sentence\n",
        "from tensorflow.keras.utils import to_categorical# to_categorical(), a numpy array (or) a vector which has integers that represent different categories, can be converted into a numpy array (or) a matrix which has binary values and has columns equal to the number of categories in the data\n",
        "from tensorflow.keras.models import Sequential# plain stack of layers where each layer has exactly one input tensor and one output tensor. \n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding#dense for hidden layers,lstm,embedding:Turns positive integers (indexes) into dense vectors of fixed size and used only in first layer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences#ensure that all sequences in a list have the same length"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRLKjK3uFc2T"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)#fit_on_texts() updates internal vocabulary based on a list of texts\n",
        "sequences = tokenizer.texts_to_sequences(lines)#transforms each text in texts to a sequence of integers"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rocOunGpGH24"
      },
      "source": [
        "sequences containes a list of integer values created by tokenizer. Each line in sequences has 51 words.split each line such that the first 50 words are in X and the last word is in y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW6BXpO1GPBe",
        "outputId": "f6e834c4-6d1b-44c9-8f17-35e1f0f9e61d"
      },
      "source": [
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:,-1]\n",
        "X[0]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   47,  1408,  1264,    37,   451,  1406,     9,  2766,  1158,\n",
              "        1213,   171,   132,   269,    20,    24,     1,  4782,    87,\n",
              "          30,    98,  4781,    18,   715,  1263,   171,   211,    18,\n",
              "         829,    20,    27,  3807,     4,   214,   121,  1212,   153,\n",
              "       13004,    31,  2765,  1847,    16, 13003, 13002,   754,     7,\n",
              "        3806,    99,  2430,   466,    31])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwR33YjqG33W"
      },
      "source": [
        "vocab_size contains all the uniques words in the dataset\n",
        "\n",
        "tokenizer.word_index gives the mapping of each unique word to its numerical equivalent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyjs3aaqG_g3"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uflWUWCZHRZi"
      },
      "source": [
        "to_categorical() converts a class vector (integers) to binary class matrix. num_classes is the total number of classes which is vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XEFEpo6HHNh"
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOvR65LyHbaw",
        "outputId": "b251679e-ce5a-449a-f72a-5c5ca4f45004"
      },
      "source": [
        "seq_length = X.shape[1]\n",
        "seq_length"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_zhJi0-Hukh"
      },
      "source": [
        "Embedding layer:\n",
        "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset. It requires 3 arguments:\n",
        "\n",
        "input_dim: This is the size of the vocabulary in the text data which is vocab_size in this case.\n",
        "\n",
        "output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word.\n",
        "\n",
        "input_length: Length of input sequences which is seq_length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzZZDHeSH3Bb"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))#return_sequence when set to True returns the full sequence as the output\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5HK3iNWIL0A",
        "outputId": "31ae5d46-f0fa-47c7-98be-2bd271d2b9cb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 50)            650450    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 13009)             1313909   \n",
            "=================================================================\n",
            "Total params: 2,115,259\n",
            "Trainable params: 2,115,259\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG0nsFNsJKrc"
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "#use categorical_crossentropy when there are multiple outputs and if its binary output then use binary_crossentroy"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGGaBQMoPPtw",
        "outputId": "3529ca49-2078-4ac1-fce0-85e17fb5bb92"
      },
      "source": [
        "model.fit(X, y, batch_size = 256, epochs = 5)#set it to 100 epoch for good accuracy\n",
        "#training the model,batch_size is 256 so the weights will be updates after 256 training examples\n",
        "#epoch is the number of passes of entire training dataset the model has completed. Its like more you do revision better you perform in exam. so more epoch means more accuracy"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 325s 416ms/step - loss: 5.9980 - accuracy: 0.0902\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 325s 416ms/step - loss: 5.7910 - accuracy: 0.0978\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 323s 414ms/step - loss: 5.6702 - accuracy: 0.1028\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 321s 411ms/step - loss: 5.5759 - accuracy: 0.1068\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 321s 411ms/step - loss: 5.4945 - accuracy: 0.1096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f910119afd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "_6g9bpL4JdTN",
        "outputId": "ec1bea1d-c959-432c-a799-0915620dc26a"
      },
      "source": [
        "#now generate the word\n",
        "#taking a random line\n",
        "new_text=lines[12343]\n",
        "new_text"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'home of love if i have ranged like him that travels i return again just to the time not with the time exchanged so that my self bring water for my stain never believe though in my nature reigned all frailties that besiege all kinds of blood that it could so'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkcHdYTXKKiX"
      },
      "source": [
        "#generate_text_seq() generates n_words number of words after the given new_text\n",
        "def generate_text_seq(model, tokenizer, text_seq_length, new_text, n_words):\n",
        "  text = []\n",
        "\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([new_text])[0]# pre-process the new_text before predicting\n",
        "    encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')#to create uniformity in text,convert the new_text to 50 words by using pad_sequences()\n",
        "  #encode the new_text using the same encoding used for encoding the training data\n",
        "    y_predict = model.predict_classes(encoded)\n",
        "\n",
        "    predicted_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == y_predict:#search the word in tokenizer using the index in y_predict\n",
        "        predicted_word = word\n",
        "        break\n",
        "    new_text = new_text + ' ' + predicted_word\n",
        "    text.append(predicted_word)#append the predicted word to new_text and text and repeat the process\n",
        "  return ' '.join(text)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Fy7nR4WnKKRo",
        "outputId": "bd9997cb-6446-4ae7-8c04-04d7f62a53f9"
      },
      "source": [
        "generate_text_seq(model, tokenizer, seq_length, new_text, 100)# next 100 words are predicted by the model for the new_text"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-54-9b3e3b2b24d5>:9: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i have not so a man and not been a man and not been a man and i have not so a man and not been a man and not been a man and not been a man and i have not so a man and not been a man and not been a man and not been a man and i have not so a man and not been a man and not been a man and not been a man and i have not so a man and not been a man and not been a man and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5dzlSmrKJ8v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}